{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In this exercise, we will learn several dimensionality reduction methods using the Penguin dataset.\n",
    "\n",
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the penguins dataset\n",
    "data_table = sns.load_dataset(\"penguins\")\n",
    "data_table.head()\n",
    "\n",
    "# Impute the missing values\n",
    "for column in data_table.columns:\n",
    "    data_table[column] = data_table[column].fillna(\n",
    "        data_table[column].dropna().mode()[0]\n",
    "    )\n",
    "\n",
    "categorical_cols = [\"island\", \"sex\"]\n",
    "prep_data_table = pd.get_dummies(data_table, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "X = prep_data_table.drop(columns=[\"species\"]).values.astype(float)\n",
    "y = prep_data_table[\"species\"].values\n",
    "feature_cols = prep_data_table.drop(columns=[\"species\"]).columns\n",
    "\n",
    "X = X.astype(float)\n",
    "ylabel, yids = np.unique(y, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "In LDA, we will \n",
    "1) Compute the within-class and between-class scatter matrices\n",
    "2) Compute the generalized eigenvalue problem to generate the eigenvectors. \n",
    "3) Project the data to 2D by using the eigenvectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "ylab = np.unique(y)\n",
    "Sw = np.zeros((X.shape[1], X.shape[1]))  # Within class covariance\n",
    "Sb = np.zeros((X.shape[1], X.shape[1]))  # Between class covariance\n",
    "\n",
    "# TODO: Compute the within-class scatter matrix\n",
    "# Hint\n",
    "# - Sume of covariances over all classes\n",
    "# - np.cov is useful to compute the covariance of each class\n",
    "ylab = np.unique(y)\n",
    "Sw = np.zeros((X.shape[1], X.shape[1]))\n",
    "for yc in ylab:\n",
    "    Xclass = X[y == yc]\n",
    "    Sw += np.cov(Xclass.T)\n",
    "\n",
    "# TODO: Compute the between-class scatter matrix\n",
    "# Hint:\n",
    "#  1. Compute the center of the data, m\n",
    "#  2. Compute the center of individual classes, mc.\n",
    "#  3. Compute the variance of mc over all classes with m as the center, namely \\sum (mc - m)(mc - m)^T\n",
    "mu = X.mean(axis=0)\n",
    "for i, yc in enumerate(ylab):\n",
    "    mc = X[y == yc].mean(axis=0)\n",
    "    Sb += np.outer((mc - mu), (mc - mu).T)\n",
    "\n",
    "\n",
    "# TODO: Compute the generalized eigenvalue problem\n",
    "w, v = sparse.linalg.eigs(Sb, M=Sw, k=2, which=\"LM\")\n",
    "\n",
    "\n",
    "# TODO: Project the data\n",
    "Xproj = X @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax = sns.scatterplot(x=Xproj[:, 0], y=Xproj[:, 1], hue=y)\n",
    "\n",
    "# Label your axis.\n",
    "ax.set_xlabel(\"LDA 1\")\n",
    "ax.set_ylabel(\"LDA 2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applsoftcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
